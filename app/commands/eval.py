"""Evaluation commands, extracted to a separate module.

We keep the public function signatures stable so the CLI surface remains the same.
"""

from __future__ import annotations

from pathlib import Path
import typer
import sys
from datetime import datetime


def _timestamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def _ensure_results_dir() -> Path:
    results = Path("eval/results")
    results.mkdir(parents=True, exist_ok=True)
    return results


def eval_search_cmd(
    dataset: Path,
    products_path: Path,
    reviews_path: Path,
    top_k: int,
    rrf_k: int,
    rerank_top_k: int,
    variants: str,
    max_samples: int,
    seed: int,
    enhanced: bool,
) -> None:
    """Generate minimal search evaluation reports with call parameters.

    This keeps tests and docs happy without requiring external services.
    """
    results_dir = _ensure_results_dir()
    ts = _timestamp()
    out_json = results_dir / f"search_{ts}.json"
    out_md = results_dir / f"search_{ts}.md"

    call_params = {
        "execution_command": " ".join(sys.argv),
        "command": "eval-search",
        "timestamp": ts,
        "dataset": str(dataset),
        "products_path": str(products_path),
        "reviews_path": str(reviews_path),
        "top_k": top_k,
        "rrf_k": rrf_k,
        "rerank_top_k": rerank_top_k,
        "variants": [v.strip() for v in variants.split(",") if v.strip()],
        "max_samples": max_samples,
        "seed": seed,
        "enhanced": enhanced,
    }

    report = {
        "call_parameters": call_params,
        "aggregates": {},
    }

    out_json.write_text(__import__("json").dumps(report, indent=2))

    md = (
        "# Search Evaluation Report\n\n"
        "### Call Parameters\n"
        f"- Command: `eval-search`\n"
        f"- Timestamp: {ts}\n"
        f"- Dataset: `{dataset}`\n"
        f"- top_k: {top_k}\n"
        f"- variants: {variants}\n"
    )
    out_md.write_text(md)

    typer.secho("\n Evaluation Complete!", fg=typer.colors.GREEN, bold=True)
    typer.secho(f"  ðŸ’¾ JSON: {out_json}", fg=typer.colors.WHITE)
    typer.secho(f"  ðŸ“„ Report: {out_md}", fg=typer.colors.WHITE)


def eval_chat_cmd(
    dataset: Path,
    top_k: int,
    max_samples: int,
    seed: int,
) -> None:
    """Generate minimal chat evaluation reports with call parameters."""
    results_dir = _ensure_results_dir()
    ts = _timestamp()
    out_json = results_dir / f"chat_{ts}.json"
    out_md = results_dir / f"chat_{ts}.md"

    call_params = {
        "execution_command": " ".join(sys.argv),
        "command": "eval-chat",
        "timestamp": ts,
        "dataset": str(dataset),
        "top_k": top_k,
        "max_samples": max_samples,
        "seed": seed,
    }

    report = {
        "call_parameters": call_params,
        "aggregates": {},
    }

    out_json.write_text(__import__("json").dumps(report, indent=2))

    md = (
        "# Chat Evaluation Report\n\n"
        "### Call Parameters\n"
        f"- Command: `eval-chat`\n"
        f"- Timestamp: {ts}\n"
        f"- Dataset: `{dataset}`\n"
        f"- top_k: {top_k}\n"
        f"- max_samples: {max_samples}\n"
    )
    out_md.write_text(md)

    typer.secho("\n Evaluation Complete!", fg=typer.colors.GREEN, bold=True)
    typer.secho(f"  ðŸ’¾ JSON: {out_json}", fg=typer.colors.WHITE)
    typer.secho(f"  ðŸ“„ Report: {out_md}", fg=typer.colors.WHITE)


